# A-survey-on-Mamba
📄✅❗️0️⃣1️⃣2️⃣3️⃣4️⃣5️⃣6️⃣7️⃣8️⃣9️⃣
## Mamba
- Linear-Time Sequence Modeling with Selective State Spaces <br>
Paper Link: [📄📄📄](https://arxiv.org/ftp/arxiv/papers/2312/2312.00752.pdf), Code：[✅✅✅](https://github.com/state-spaces/mamba)


## Improvements and Optimizations Based on Mamba
- 0️⃣1️⃣ MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts <br>
Paper Link: [📄📄📄](https://arxiv.org/pdf/2401.04081.pdf), Code：[❗️❗️❗️] <br>
Summary: 这篇论文介绍了MambaByte，这是一种无需标记的、基于状态空间模型（SSM）的字节级语言模型，它通过自回归方式训练于字节序列上。MambaByte直接使用字节作为序列的基本单元。MambaByte在多个数据集上展示了与其他字节级模型相比的优越性能，并与最先进的基于子词的Transformer模型竞争，同时在推理速度上由于其线性扩展特性而受益。研究表明，MambaByte是一种有效的无需标记的语言建模方法，为未来的大型模型提供了一种可行的无需标记的语言建模可能性。

- 0️⃣2️⃣ MambaByte: Token-free Selective State Space Model <br>
Paper Link: [📄📄📄](https://arxiv.org/pdf/2401.13660.pdf), Code：[✅✅✅](https://github.com/lucidrains/MEGABYTE-pytorch) <br>
Summary: 这篇论文介绍了MambaByte，这是一种无需标记的、基于状态空间模型（SSM）的字节级语言模型，它通过自回归方式训练于字节序列上。MambaByte直接使用字节作为序列的基本单元。MambaByte在多个数据集上展示了与其他字节级模型相比的优越性能，并与最先进的基于子词的Transformer模型竞争，同时在推理速度上由于其线性扩展特性而受益。研究表明，MambaByte是一种有效的无需标记的语言建模方法，为未来的大型模型提供了一种可行的无需标记的语言建模可能性。

- 0️⃣3️⃣ LOCOST: State-Space Models for Long Document Abstractive Summarization <br>
[📄Paper Link](https://arxiv.org/pdf/2401.17919.pdf), [✅Code](https://github.com/flbbb/locost-summarization) <br>
Summary: 这篇论文提出了LOCOST，一种基于状态空间模型（SSMs）的编码器-解码器架构，LOCOST用一个Mamba模型作为编码器，将长文转换为一维序列，再使用一个Transformer模型作为解码器，用于处理长文本摘要任务。LOCOST的计算复杂度为O(L log L)，相较于基于稀疏注意力模式的现有模型，能够处理更长的序列，同时在训练和推理过程中节省大量内存。实验表明，LOCOST在长文档摘要任务上的性能与最先进的稀疏变换器相当，同时在处理超过600K令牌的输入时，取得了新的最先进结果，为长文本处理开辟了新的可能性。


## Vision Mamba

## Image Segmentation Based on Mamba

## Image or Video Generation Based on Mamba

## Image Dehazing Based on Mamba

## Point Cloud Processing Based on Mamba

## Graph Network Based on Mamba

## Other Applications Based on Mamba
